{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**General Template for Prompt**"
      ],
      "metadata": {
        "id": "u-bdE_Lr_sP_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The general prompt is provided below which is used for zero-shot training for all models below. All API keys can be stored in the left secrets tab for direct access.\n",
        "\n",
        "Represented Models: GPT-4, Claude-3 Opus, MistralAI, Code Llama, Facebook Llama-2, Galactica, ChemLLM,"
      ],
      "metadata": {
        "id": "EN3cO-6AClck"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "general_template = \"You are an expert chemist. Given the SMILES representation of reactants and reagents, your task is to predict the potential product using your chemical reaction knowledge.\"\n",
        "task_specific_template = (\n",
        "    \"The input contains both reactants and reagents, and different reactants and reagents are separated by \\\".\\\". \"\n",
        "    \"Your reply should contain only the SMILES representation of the predicted product and no other text. \"\n",
        "    \"Your reply must be valid and chemically reasonable.\"\n",
        ")\n",
        "icl_example = (\n",
        "    \"Reactants and reagents SMILES: C1COC1.CCN(CC)CC.CS(=O)(=O)Cl.CS(C)=O.N[C@@H]C2=CC=C(CN3C=C(CO)C(C(F)(F)F)=N3)C=C2C1\\n\"\n",
        "    \"Product SMILES: CS(=O)(=O)N[C@@H]1CC2=CC=C(CN3C=C(CO)C(C(F)(F)F)=N3)C=C2C1\"\n",
        ")\n",
        "question = \"Reactants and reagents SMILES: CCN.CN1C=CC=C1C=O\\nProduct SMILES:\"\n",
        "\n",
        "prompt = f\"{general_template}\\n\\n{task_specific_template}\\n\\nICL Example:\\n{icl_example}\\n\\n{question}\""
      ],
      "metadata": {
        "id": "t9diyTyf_vwU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "# for access to api keys stored in secrets tab\n",
        "import secrets"
      ],
      "metadata": {
        "id": "_NDrVRp-AwnX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**GPT-4 Zero-Shot**"
      ],
      "metadata": {
        "id": "XxI-zHq__IRD"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WQVqIWjc8YaF"
      },
      "outputs": [],
      "source": [
        "!pip install openai\n",
        "import os\n",
        "from openai import OpenAI"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "client = OpenAI(\n",
        "    api_key=secrets.OPENAI_API_KEY,\n",
        ")"
      ],
      "metadata": {
        "id": "mORCa7kgAR4z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "response = openai.ChatCompletion.create(\n",
        "    model=\"gpt-4\",\n",
        "    messages=[{\"role\": \"user\", \"content\": prompt}]\n",
        "    )"
      ],
      "metadata": {
        "id": "M03Iq8Ba-xYa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(response)\n",
        "print(response['choices'][0]['message']['content'])"
      ],
      "metadata": {
        "id": "NOVPm7Xa-yfe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Claude-3 Opus Zero-Shot**"
      ],
      "metadata": {
        "id": "88MtgrSpBAe4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install anthropic\n",
        "import os\n",
        "from anthropic import Anthropic"
      ],
      "metadata": {
        "id": "zjWYWRdFBGtS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "client = Anthropic(\n",
        "    api_key=secrets.ANTHROPIC_API_KEY,\n",
        ")"
      ],
      "metadata": {
        "id": "AHpaEWV4BqJz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "message = client.messages.create(\n",
        "    max_tokens=1024,\n",
        "    messages=[\n",
        "        {\n",
        "            \"role\": \"user\",\n",
        "            \"content\": \"Hello, Claude\",\n",
        "        },\n",
        "        {\n",
        "            \"role\": \"user\",\n",
        "            \"content\": prompt,\n",
        "        },\n",
        "\n",
        "    ],\n",
        "    model=\"claude-3-opus-20240229\",\n",
        ")\n",
        "print(message.content)"
      ],
      "metadata": {
        "id": "4WLl2opRB23m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**MistralAI Zero-Shot**"
      ],
      "metadata": {
        "id": "uKpCJ6MICh7p"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "! pip install mistralai\n",
        "from mistralai import Mistral\n",
        "api_key = secrets.MISTRAL_API_KEY"
      ],
      "metadata": {
        "id": "YiFZtsKJCkrY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = \"mistral-large-latest\"\n",
        "\n",
        "client = Mistral(api_key=api_key)\n",
        "\n",
        "chat_response = client.chat.complete(\n",
        "    model=model,\n",
        "    messages=[{\"role\":\"user\", \"content\":prompt}]\n",
        ")\n",
        "\n",
        "print(chat_response.choices[0].message.content)"
      ],
      "metadata": {
        "id": "9YIIW262FsW1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Code Llama Zero-Shot**"
      ],
      "metadata": {
        "id": "035b2z3SGLwc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install replicate\n",
        "import replicate\n",
        "export REPLICATE_API_TOKEN = secrets.REPLICATE_API_KEY"
      ],
      "metadata": {
        "id": "uWxXlq7tGJ65"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "output = replicate.run(\n",
        "    \"meta/codellama-70b-instruct:a279116fe47a0f65701a8817188601e2fe8f4b9e04a518789655ea7b995851bf\",\n",
        "    input={\n",
        "        \"prompt\": prompt,\n",
        "      }\n",
        ")\n",
        "print(\"\".join(output))"
      ],
      "metadata": {
        "id": "j1rJEy2SGqbB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Facebook Llama-2 Zero-Shot**"
      ],
      "metadata": {
        "id": "sWyTv-ePGx_-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from huggingface_hub import InferenceClient\n",
        "\n",
        "client = InferenceClient(\n",
        "    \"meta-llama/Llama-2-7b-chat-hf\",\n",
        "    token=secrets.LLAMA2_API_KEY, # THIS NEEDS TO BE GENERATED IN HUGGINGFACE AT THIS LINK https://huggingface.co/meta-llama/Llama-2-7b-chat-hf\n",
        ")"
      ],
      "metadata": {
        "id": "ifDFa34xH4jM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for message in client.chat_completion(\n",
        "\tmessages=[{\"role\": \"user\", \"content\": prompt}],\n",
        "\tmax_tokens=500,\n",
        "\tstream=True,\n",
        "):\n",
        "    print(message.choices[0].delta.content, end=\"\")"
      ],
      "metadata": {
        "id": "X_JYjDlFH9AP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Galactica Zero-Shot**"
      ],
      "metadata": {
        "id": "H-XGHou7IQcO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install galai\n",
        "import galai as gal\n",
        "from galai.notebook_utils import *"
      ],
      "metadata": {
        "id": "XDw-zVRsIXwr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = gal.load_model(\"huge\", parallelize=True)"
      ],
      "metadata": {
        "id": "r8nr7coUI0D8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input_prompt = prompt\n",
        "\n",
        "reference = model.generate_reference(input_prompt)\n",
        "display_markdown(f\"**Prompt**: {input_prompt}\\n\\n**Reference**: {reference}\")"
      ],
      "metadata": {
        "id": "SR20Nue5JN7r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**ChemLLM Zero-Shot**"
      ],
      "metadata": {
        "id": "KwtvYofwLERb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Website Available for Zero-Shot: https://chemllm.org/\n",
        "# https://huggingface.co/AI4Chem/ChemLLM-7B-Chat\n",
        "!pip install transformers\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer, GenerationConfig\n",
        "import torch\n",
        "\n",
        "model_name_or_id = \"AI4Chem/ChemLLM-7B-Chat\"\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(model_name_or_id, torch_dtype=torch.float16, device_map=\"auto\",trust_remote_code=True)\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name_or_id,trust_remote_code=True)\n",
        "\n",
        "input_prompt = prompt\n",
        "\n",
        "inputs = tokenizer(input_prompt, return_tensors=\"pt\").to(\"cuda\")"
      ],
      "metadata": {
        "id": "Y52kKwBiMWVs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "generation_config = GenerationConfig(\n",
        "    do_sample=True,\n",
        "    top_k=1,\n",
        "    temperature=0.9,\n",
        "    max_new_tokens=500,\n",
        "    repetition_penalty=1.5,\n",
        "    pad_token_id=tokenizer.eos_token_id\n",
        ")\n",
        "\n",
        "outputs = model.generate(**inputs, generation_config=generation_config)\n",
        "print(tokenizer.decode(outputs[0], skip_special_tokens=True))"
      ],
      "metadata": {
        "id": "IlkAjGbpMhfu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Molonist LLM**"
      ],
      "metadata": {
        "id": "lDPTkEJwMvzM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Need to run locally from computer\n",
        "\n",
        "# STEP 1\n",
        "# >> git clone https://github.com/zjunlp/Mol-Instruction\n",
        "# >> cd demo\n",
        "\n",
        "# STEP 2\n",
        "# ! pip install gradio\n",
        "\n",
        "# STEP 3\n",
        "# SPECIFY LOCAL PARAMS IN generate.sh FILE\n",
        "\n",
        "# >> CUDA_VISIBLE_DEVICES=0 python generate.py \\\n",
        "#     --CLI False\\\n",
        "#     --protein False\\\n",
        "#     --load_8bit \\\n",
        "#     --base_model $BASE_MODEL_PATH \\\n",
        "#     --share_gradio True\\\n",
        "#     --lora_weights $FINETUNED_MODEL_PATH \\\n",
        "\n",
        "# STEP 4\n",
        "# SET FINETUNED_MODEL_PATH to 'zjunlp/llama-molinst-molecule-7b'\n",
        "\n",
        "# STEP 5\n",
        "# >> sh generate.sh\n",
        "\n",
        "# STEP 6\n",
        "# >> python generate.py --CLI True\n",
        "\n",
        "# Command-Line based interaction will now be available with model"
      ],
      "metadata": {
        "id": "WLEpZ4cSMqA-"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**LLASMOL-Galactica, LLASMOL-LLAMA2, LLASMOL-CODE_LLAMA, LLASMOL-MISTRAL**\n",
        "Needs to be run locally from computer"
      ],
      "metadata": {
        "id": "FeFn61klOnN7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# clone https://github.com/OSU-NLP-Group/LLM4Chem to local repo\n",
        "# CD into folder\n",
        "\n",
        "# from generation import LlaSMolGeneration\n",
        "\n",
        "# generator = LlaSMolGeneration('osunlp/LlaSMol-Mistral-7B')\n",
        "# generator.generate('Can you tell me the IUPAC name of <SMILES> C1CCOC1 </SMILES> ?')\n",
        "\n",
        "# Above, the model can be switched out among these 4 following models: \"LlaSMol-Mistral-7B\", \"LlaSMol-CodeLlama-7B\", \"LlaSMol-Llama2-7B\", \"LlaSMol-Galactica-6.7B\"\n",
        "# Smiles representation should be wrapped as following <SMILES> ... </SMILES>"
      ],
      "metadata": {
        "id": "TO7LXuiCOqbn"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}